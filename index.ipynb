{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid-19 Time Series Modeling\n",
    "\n",
    "- Flex\n",
    "- Instructor: Morgan Jones\n",
    "- Blog: [https://datascienceprojectsandmore.blogspot.com/](https://datascienceprojectsandmore.blogspot.com/)\n",
    "- Date of Review: Tuesday, April 9th 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "This project creates a model that forecasts the number of deaths by Covid-19 given the historical data.  The Covid-19 pandemic was detrimental in the number of people who perished from this disease.  The idea behind this project is that if we can create a predictive model that will forecast the amount of harm, then public health officials can be better informed, advised, and empowered to mitigate future deaths.  Specifically, this project focuses on a continent-wide evaluation, looking at Asia.\n",
    "\n",
    "This project goes through a model iteration process, starting with a naive time series model, and iterating through ARIMA, multivariate ARIMA, Linear Regression, Facebook's Prophet, and multivariate prophet models.  The project finalizes on the multivariate prophet model as the final model based on the evaluation metric, root mean squared error, and evaluates this model on the holdout test set.  Lastly, the project offers a few recommendations to the proposed business and next steps for future projects.\n",
    "\n",
    "- [GitHub README]()\n",
    "- [non-technical presentation]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## The Covid-19 Pandemic\n",
    "\n",
    "![covid_map](images/covid_map.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Covid-19 pandemic needs little introduction as it left no corner of our world untouched.  It devastated lives, and economies -- day-to-day life was dramatically altered for about 2-3 years.  During the pandemic, the World Health Organization, along with many other public health entities, concerned themselves with not only reacting to this public health crisis but also to recording and analyzing the data from the outbreak. This data accumulation was not only a way to evaluate the devastation and how different countries handled the situation, but also to allowed them to provide better recommendations and make impactful decisions concerning public health. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  This Project | The Business + Business Problem\n",
    "\n",
    "The specific business for this project is one akin to the World Health Organization (WHO) that 'treats data as a public good'([WHO Principles](https://data.who.int/about/data/who-data-principles), and also wants to leverage this data to make recommendations to public health officials.  When the pandemic arose, the WHO aided in the response including \"facilitating research, develping guidance, coordinating vaccine development and distribution, and monitoring daily case numbers and trends around the world\" ([WHO_COVID-19](https://www.who.int/news-room/fact-sheets/detail/coronavirus-disease-(covid-19)). Another company, Our World in Data, has extensive documentation of the Covid-19 pandemic ([Our World in Data/coronavirus](https://ourworldindata.org/coronavirus)).\n",
    "\n",
    "The idea for this project is to create a model that will forecast how much worse the pandemic will be in order to advise public health officials and policy makers on how to best handle the situation such that damages are minimized. We will use the number of deaths as the target to forecast and evaluate how harmful the outbreak is (or will be for predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Source\n",
    "\n",
    "The Data from this project is sourced directly from Kaggle, a dataset called [Our World in Data - COVID-19](https://www.kaggle.com/datasets/caesarmario/our-world-in-data-covid19-dataset).  The company, [Our World in Data](https://ourworldindata.org/explorers/coronavirus-data-explorer) combined their own data along with data from the John Hopkins University and the WHO.  Further details on their data and sources can be found on the [GitHub page here](https://github.com/owid/covid-19-data/blob/master/public/data/README.md).\n",
    "\n",
    "For visual purposes, the project also utilizes a dataset of latitude and longitude data of countries from Kaggle, originally sourced from google.  See that data set [here](https://www.kaggle.com/datasets/paultimothymooney/latitude-and-longitude-for-every-country-and-state)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducing via Google CoLabs\n",
    "\n",
    "If you are reproducing this project via Google CoLabs, please uncomment the two `!pip install` commands in the imports cell below, along with the 3 cells of code below the versions print outs.  The pip install commands will allow the use of pmdarima and Prophet libraries.  The three cells to uncomment will configure the Google CoLab environment to download the data sources from Kaggle, however **be sure to enter your own Kaggle username and API**, otherwise this cell will not work.  After configuring the cells download the two datasets and unzip them for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Needed Libraries\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('dark')\n",
    "\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "# For Google CoLab might need to install pmdarima and Prophet\n",
    "#!pip install pmdarima\n",
    "#!pip install Prophet\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "from prophet import Prophet\n",
    "from prophet.utilities import regressor_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python |  3.8.5\n",
      "Pandas |  1.1.3\n",
      "Numpy |  1.24.4\n",
      "SciPy |  1.10.1\n",
      "Matplotlib |  3.3.1\n",
      "Seaborn |  0.11.0\n",
      "Geopandas |  0.13.2\n",
      "SciKit Learn |  1.3.2\n",
      "Stats Models |  0.14.1\n",
      "pmdarima |  2.0.4\n",
      "Prophet |  1.1.5\n"
     ]
    }
   ],
   "source": [
    "# Versions\n",
    "import sys\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import statsmodels as sm\n",
    "import prophet\n",
    "import pmdarima\n",
    "\n",
    "print('Python | ', sys.version[:5])\n",
    "print('Pandas | ', pd.__version__)\n",
    "print('Numpy | ', np.__version__)\n",
    "print('SciPy | ', scipy.__version__)\n",
    "print('Matplotlib | ', matplotlib.__version__)\n",
    "print('Seaborn | ', sns.__version__)\n",
    "print('Geopandas | ', gpd.__version__)\n",
    "print('SciKit Learn | ', sklearn.__version__)\n",
    "print('Stats Models | ', sm.__version__)\n",
    "print('pmdarima | ', pmdarima.__version__)\n",
    "print('Prophet | ', prophet.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep Google CoLab environment to download data from Kaggle\n",
    "#!mkdir ~/.kaggle\n",
    "#!touch ~/.kaggle/kaggle.json\n",
    "\n",
    "#username = ''  ## Your Kaggle username\n",
    "#api_key = ''  ## Your Kaggle API key\n",
    "\n",
    "#api_token = {\"username\": username,\n",
    "#             \"key\": api_key}\n",
    "\n",
    "#with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
    "#    json.dump(api_token, file)\n",
    "\n",
    "#!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset from Kaggle\n",
    "#!kaggle datasets download -d caesarmario/our-world-in-data-covid19-dataset\n",
    "#!kaggle datasets download -d paultimothymooney/latitude-and-longitude-for-every-country-and-state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell unzips the downloaded data\n",
    "#shutil.unpack_archive('our-world-in-data-covid19-dataset.zip', '/content')\n",
    "#shutil.unpack_archive('latitude-and-longitude-for-every-country-and-state.zip', '/content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "First, we look at the complete data and find that there are quite a few redundancies in the accumulated data.  With the idea of modeling in mind, many of the columns are dropped.  For example, the data includes `total_cases`, `new_cases`, `new_cases_smoothed`, `new_cases_per_million`, and `new_cases_smoothed_per_million`.  For modeling, specifically for time series modeling, we are interested in the daily reports, and thus keep `new_cases_smoothed` because (a) smoothed data is better for seeing trends and (b) we want to work with the real data of the number of new cases that occurred each day.  This logic is applied to many of the variables in the original data set.  See below for a list of the variables kept and their definitions.  See also the [GitHub page](https://github.com/owid/covid-19-data/blob/master/public/data/README.md) for a full list of all the variables and their descriptions. \n",
    "\n",
    "Next, a visual of the countries in which the data is coming from is produced. The idea behind this is to try and see the best way to splice the data -- is it best by individual country, continent or by something else?  After looking at this map, it is decided to break up the data by continent and focus on Asia.  *(The original idea was to create models for each continent, but this project only had the bandwidth to look at one.  Future work would be to expand this to the other continents and compare.)* \n",
    "\n",
    "The project then transitions into a data cleaning phase in order to deal with the missing values in the Asia subset before looking at other 'data exploration' visuals.  The reasoning behind this is we want to explore the dataset that will be used in modeling; however this needs to be created by aggregating the Asia subset such that there is only one entry per day.  The missing values must be dealt with prior to aggregating.  See below for further details that occur during the Data Cleaning phase. \n",
    "\n",
    "> One note concerning the data cleaning phase.  A limitation of this project in specific is not only the amount of missing data in the raw data source, but also the question behind the validity of the data itself.  While the WHO is a very reputable source and is to be trusted, it is widely known that countries in Asia, namely China and India, were suspected of purposefully mis-reporting deaths due to Covid-19.  It is important to keep in mind that the model can only ever be as good as the data used.  (see [China is 'heavily underreporting' ...](https://abcnews.go.com/Health/china-heavily-underreporting-number-covid-19-deaths/story?id=96389383), [China deletes Covid-19 death data](https://www.ft.com/content/a634d844-5298-441b-b2e8-0eabe0b3c1d7), [WHO says China revised...](https://www.cnbc.com/2020/04/17/who-says-china-revised-coronavirus-infection-data-to-leave-no-case-undocumented.html), [COVID mortality in India](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9836201/), and [WHO says millions of Covid deaths...](https://timesofindia.indiatimes.com/india/who-says-millions-of-covid-deaths-went-unreported-in-india-centre-strongly-objects-methodology-key-points/articleshow/91349479.cms) for further reading).\n",
    "\n",
    "Lastly, the project goes over some visuals for the aggregated Asia subset data that will be used for modeling, including showing the target variable that will be used for the strictly time series models (as well as all the models), the nubmer of new deaths per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 350085 entries, 0 to 350084\n",
      "Data columns (total 67 columns):\n",
      " #   Column                                      Non-Null Count   Dtype  \n",
      "---  ------                                      --------------   -----  \n",
      " 0   iso_code                                    350085 non-null  object \n",
      " 1   continent                                   333420 non-null  object \n",
      " 2   location                                    350085 non-null  object \n",
      " 3   date                                        350085 non-null  object \n",
      " 4   total_cases                                 312088 non-null  float64\n",
      " 5   new_cases                                   340457 non-null  float64\n",
      " 6   new_cases_smoothed                          339198 non-null  float64\n",
      " 7   total_deaths                                290501 non-null  float64\n",
      " 8   new_deaths                                  340511 non-null  float64\n",
      " 9   new_deaths_smoothed                         339281 non-null  float64\n",
      " 10  total_cases_per_million                     312088 non-null  float64\n",
      " 11  new_cases_per_million                       340457 non-null  float64\n",
      " 12  new_cases_smoothed_per_million              339198 non-null  float64\n",
      " 13  total_deaths_per_million                    290501 non-null  float64\n",
      " 14  new_deaths_per_million                      340511 non-null  float64\n",
      " 15  new_deaths_smoothed_per_million             339281 non-null  float64\n",
      " 16  reproduction_rate                           184817 non-null  float64\n",
      " 17  icu_patients                                37615 non-null   float64\n",
      " 18  icu_patients_per_million                    37615 non-null   float64\n",
      " 19  hosp_patients                               38902 non-null   float64\n",
      " 20  hosp_patients_per_million                   38902 non-null   float64\n",
      " 21  weekly_icu_admissions                       10205 non-null   float64\n",
      " 22  weekly_icu_admissions_per_million           10205 non-null   float64\n",
      " 23  weekly_hosp_admissions                      23253 non-null   float64\n",
      " 24  weekly_hosp_admissions_per_million          23253 non-null   float64\n",
      " 25  total_tests                                 79387 non-null   float64\n",
      " 26  new_tests                                   75403 non-null   float64\n",
      " 27  total_tests_per_thousand                    79387 non-null   float64\n",
      " 28  new_tests_per_thousand                      75403 non-null   float64\n",
      " 29  new_tests_smoothed                          103965 non-null  float64\n",
      " 30  new_tests_smoothed_per_thousand             103965 non-null  float64\n",
      " 31  positive_rate                               95927 non-null   float64\n",
      " 32  tests_per_case                              94348 non-null   float64\n",
      " 33  tests_units                                 106788 non-null  object \n",
      " 34  total_vaccinations                          79308 non-null   float64\n",
      " 35  people_vaccinated                           75911 non-null   float64\n",
      " 36  people_fully_vaccinated                     72575 non-null   float64\n",
      " 37  total_boosters                              47562 non-null   float64\n",
      " 38  new_vaccinations                            65346 non-null   float64\n",
      " 39  new_vaccinations_smoothed                   180718 non-null  float64\n",
      " 40  total_vaccinations_per_hundred              79308 non-null   float64\n",
      " 41  people_vaccinated_per_hundred               75911 non-null   float64\n",
      " 42  people_fully_vaccinated_per_hundred         72575 non-null   float64\n",
      " 43  total_boosters_per_hundred                  47562 non-null   float64\n",
      " 44  new_vaccinations_smoothed_per_million       180718 non-null  float64\n",
      " 45  new_people_vaccinated_smoothed              180489 non-null  float64\n",
      " 46  new_people_vaccinated_smoothed_per_hundred  180489 non-null  float64\n",
      " 47  stringency_index                            197651 non-null  float64\n",
      " 48  population_density                          297178 non-null  float64\n",
      " 49  median_age                                  276367 non-null  float64\n",
      " 50  aged_65_older                               266708 non-null  float64\n",
      " 51  aged_70_older                               273597 non-null  float64\n",
      " 52  gdp_per_capita                              270863 non-null  float64\n",
      " 53  extreme_poverty                             174561 non-null  float64\n",
      " 54  cardiovasc_death_rate                       271487 non-null  float64\n",
      " 55  diabetes_prevalence                         285303 non-null  float64\n",
      " 56  female_smokers                              203659 non-null  float64\n",
      " 57  male_smokers                                200889 non-null  float64\n",
      " 58  handwashing_facilities                      132973 non-null  float64\n",
      " 59  hospital_beds_per_thousand                  239669 non-null  float64\n",
      " 60  life_expectancy                             322072 non-null  float64\n",
      " 61  human_development_index                     263138 non-null  float64\n",
      " 62  population                                  350085 non-null  float64\n",
      " 63  excess_mortality_cumulative_absolute        12184 non-null   float64\n",
      " 64  excess_mortality_cumulative                 12184 non-null   float64\n",
      " 65  excess_mortality                            12184 non-null   float64\n",
      " 66  excess_mortality_cumulative_per_million     12184 non-null   float64\n",
      "dtypes: float64(62), object(5)\n",
      "memory usage: 179.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Look at data\n",
    "df = pd.read_csv('owid-covid-data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Columns | Description | \n",
    "|:--|:--| \n",
    "| `continent` | Continent of the geographical location | \n",
    "| `location` | Geographical location | \n",
    "| `date` | Date of observation | \n",
    "| `new_cases` | New confirmed cases of COVID-19. Counts can include probable cases, where reported. In rare cases where our source reports a negative daily change due to a data correction, we set this metric to NA | \n",
    "| `new_cases_smoothed` | New confirmed cases of COVID-19 (7-day smoothed). Counts can include probable cases, where reported. | \n",
    "| `new_deaths` | New deaths attributed to COVID-19. Counts can include probable deaths, where reported. In rare cases where our source reports a negative daily change due to a data correction, we set this metric to NA. |\n",
    "| `new_deaths_smoothed` | New deaths attributed to COVID-19 (7-day smoothed). Counts can include probable deaths, where reported. |\n",
    "| `reproduction_rate` | Real-time estimate of the effective reproduction rate (R) of COVID-19. See https://github.com/crondonm/TrackingR/tree/main/Estimates-Database | \n",
    "| `icu_patients` | Number of COVID-19 patients in intensive care units (ICUs) on a given day. |\n",
    "| `hosp_patients` | Number of COVID-19 patients in hospital on a given day. |\n",
    "| `new_tests` | New tests for COVID-19 (only calculated for consecutive days). |\n",
    "| `new_tests_smoothed` | New tests for COVID-19 (7-day smoothed). For countries that don't report testing data on a daily basis, we assume that testing changed equally on a daily basis over any periods in which no data was reported. This produces a complete series of daily figures, which is then averaged over a rolling 7-day window |\n",
    "| `positive_rate` | The share of COVID-19 tests that are positive, given as a rolling 7-day average (this is the inverse of tests_per_case). |\n",
    "| `test_units` | Units used by the location to report its testing data. A country file can't contain mixed units. All metrics concerning testing data use the specified test unit. Valid units are 'people tested' (number of people tested), 'tests performed' (number of tests performed. a single person can be tested more than once in a given day) and 'samples tested' (number of samples tested. In some cases, more than one sample may be required to perform a given test.) | \n",
    "| `people_vaccinated` | Total number of people who received at least one vaccine dose | \n",
    "| `new_people_vaccinated_smoothed` | Daily number of people receiving their first vaccine dose (7-day smoothed) | \n",
    "| `new_vaccinations` | New COVID-19 vaccination doses administered (only calculated for consecutive days) | \n",
    "| `new_vaccinations_smoothed` | New COVID-19 vaccination doses administered (7-day smoothed). For countries that don't report vaccination data on a daily basis, we assume that vaccination changed equally on a daily basis over any periods in which no data was reported. This produces a complete series of daily figures, which is then averaged over a rolling 7-day window |\n",
    "| `total_boosters` | Total number of COVID-19 vaccination booster doses administered (doses administered beyond the number prescribed by the vaccination protocol) |\n",
    "| `stringency_index` | Government Response Stringency Index: composite measure based on 9 response indicators including school closures, workplace closures, and travel bans, rescaled to a value from 0 to 100 (100 = strictest response). | \n",
    "| `population_density` | Number of people divided by land area, measured in square kilometers, most recent year available | \n",
    "| `median_age` | Median age of the population, UN projection for 2020 | \n",
    "| `aged_70_older` | Share of the population that is 70 years and older in 2015 |\n",
    "| `gdp_per_capita` | Gross domestic product at purchasing power parity (constant 2011 international dollars), most recent year available | \n",
    "| `extreme_poverty` | Share of the population living in extreme poverty, most recent year available since 2010 | \n",
    "| `cardiovasc_death_rate` | Death rate from cardiovascular disease in 2017 (annual number of deaths per 100,000 people) | \n",
    "| `diabetes_prevalence` | Dabetes prevalence (% of population aged 20 to 79) in 2017 |\n",
    "| `female_smokers` | Share of women who smoke, most recent year available | \n",
    "| `male_smokers` | Share of men who smoke, most recent year available | \n",
    "| `handwashing_facilities` | Share of the population with basic handwashing facilities on premises, most recent year available |\n",
    "| `hospital_beds_per_thousand` | Hospital beds per 1,000 people, most recent year available since 2010 |\n",
    "| `life_expectancy` | Life expectancy at birth in 2019 | \n",
    "| `human_development_index` | A composite index measuring average achievement in three basic dimensions of human developmentâ€”a long and healthy life, knowledge and a decent standard of living. Values for 2019, imported from http://hdr.undp.org/en/indicators/137506 | \n",
    "| `population` | Population (latest available values). See https://github.com/owid/covid-19-data/blob/master/scripts/input/un/population_latest.csv for full list of sources | \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which columns to use/drop\n",
    "cols_to_use = ['continent', 'location', 'date', 'new_cases_smoothed', 'new_deaths_smoothed',\n",
    "               'reproduction_rate', 'icu_patients', 'hosp_patients', 'new_tests_smoothed', \n",
    "               'positive_rate', 'tests_units', 'people_vaccinated', 'new_people_vaccinated_smoothed',\n",
    "               'total_boosters', 'new_vaccinations_smoothed', 'stringency_index', 'population_density', \n",
    "               'median_age', 'aged_70_older', 'gdp_per_capita', 'extreme_poverty', 'cardiovasc_death_rate',\n",
    "               'diabetes_prevalence', 'female_smokers', 'male_smokers', 'handwashing_facilities', \n",
    "               'hospital_beds_per_thousand', 'life_expectancy', 'human_development_index', 'population']\n",
    "df1 = df[cols_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 350085 entries, 0 to 350084\n",
      "Data columns (total 30 columns):\n",
      " #   Column                          Non-Null Count   Dtype  \n",
      "---  ------                          --------------   -----  \n",
      " 0   continent                       333420 non-null  object \n",
      " 1   location                        350085 non-null  object \n",
      " 2   date                            350085 non-null  object \n",
      " 3   new_cases_smoothed              339198 non-null  float64\n",
      " 4   new_deaths_smoothed             339281 non-null  float64\n",
      " 5   reproduction_rate               184817 non-null  float64\n",
      " 6   icu_patients                    37615 non-null   float64\n",
      " 7   hosp_patients                   38902 non-null   float64\n",
      " 8   new_tests_smoothed              103965 non-null  float64\n",
      " 9   positive_rate                   95927 non-null   float64\n",
      " 10  tests_units                     106788 non-null  object \n",
      " 11  people_vaccinated               75911 non-null   float64\n",
      " 12  new_people_vaccinated_smoothed  180489 non-null  float64\n",
      " 13  total_boosters                  47562 non-null   float64\n",
      " 14  new_vaccinations_smoothed       180718 non-null  float64\n",
      " 15  stringency_index                197651 non-null  float64\n",
      " 16  population_density              297178 non-null  float64\n",
      " 17  median_age                      276367 non-null  float64\n",
      " 18  aged_70_older                   273597 non-null  float64\n",
      " 19  gdp_per_capita                  270863 non-null  float64\n",
      " 20  extreme_poverty                 174561 non-null  float64\n",
      " 21  cardiovasc_death_rate           271487 non-null  float64\n",
      " 22  diabetes_prevalence             285303 non-null  float64\n",
      " 23  female_smokers                  203659 non-null  float64\n",
      " 24  male_smokers                    200889 non-null  float64\n",
      " 25  handwashing_facilities          132973 non-null  float64\n",
      " 26  hospital_beds_per_thousand      239669 non-null  float64\n",
      " 27  life_expectancy                 322072 non-null  float64\n",
      " 28  human_development_index         263138 non-null  float64\n",
      " 29  population                      350085 non-null  float64\n",
      "dtypes: float64(26), object(4)\n",
      "memory usage: 80.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-ff5bda607d92>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['date'] = pd.to_datetime(df1['date'])\n"
     ]
    }
   ],
   "source": [
    "# Update date variable to datetime\n",
    "df1['date'] = pd.to_datetime(df1['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42.546245</td>\n",
       "      <td>1.601554</td>\n",
       "      <td>Andorra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.424076</td>\n",
       "      <td>53.847818</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.939110</td>\n",
       "      <td>67.709953</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.060816</td>\n",
       "      <td>-61.796428</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.220554</td>\n",
       "      <td>-63.068615</td>\n",
       "      <td>Anguilla</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latitude  longitude               country\n",
       "0  42.546245   1.601554               Andorra\n",
       "1  23.424076  53.847818  United Arab Emirates\n",
       "2  33.939110  67.709953           Afghanistan\n",
       "3  17.060816 -61.796428   Antigua and Barbuda\n",
       "4  18.220554 -63.068615              Anguilla"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import latitude & longitude data\n",
    "lat_long = pd.read_csv('world_country_and_usa_states_latitude_and_longitude_values.csv')\n",
    "lat_long = lat_long[['latitude', 'longitude', 'country']]\n",
    "lat_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes so can visualize\n",
    "df2 = pd.merge(df1,\n",
    "              lat_long,\n",
    "              left_on='location',\n",
    "              right_on='country',\n",
    "              how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop not countries\n",
    "loc_to_drop = ['Africa', 'Asia', 'Europe', 'European Union', 'High income', \n",
    "               'Low income', 'Lower middle income', 'North America', 'Oceania', \n",
    "               'South America', 'Upper middle income', 'World']\n",
    "\n",
    "df2 = df2[~df2['location'].isin(loc_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-940db7fa8124>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# See https://stackoverflow.com/a/53233489 for source\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgeometry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mPoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mxy\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'longitude'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latitude'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mgdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGeoDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'longitude'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'latitude'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeometry\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-940db7fa8124>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# See https://stackoverflow.com/a/53233489 for source\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgeometry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mPoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mxy\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'longitude'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latitude'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mgdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGeoDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'longitude'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'latitude'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeometry\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\shapely\\geometry\\point.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mcoords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mgeom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshapely\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid values passed to Point constructor\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\shapely\\decorators.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marray_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_flag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_flags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\shapely\\creation.py\u001b[0m in \u001b[0;36mpoints\u001b[1;34m(coords, y, z, indices, out, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mcoords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_xyz_to_coords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msimple_geometries_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGeometryType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOINT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Visualize where, which countries, the data is coming from\n",
    "    # See https://stackoverflow.com/a/53233489 for source\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(df2['longitude'], df2['latitude'])]\n",
    "gdf = GeoDataFrame(df2[['longitude', 'latitude']], geometry=geometry)   \n",
    "\n",
    "# A simple map that goes with geopandas\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "gdf.plot(ax=world.plot(figsize=(10, 6)), marker='o', color='red', markersize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above graph on where the data is coming from -- the most logical and least inhibited route will be to consolidate the data by continent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-countries from df1\n",
    "df1 = df1[~df1['location'].isin(loc_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "df1['continent'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are all the continents present\n",
    "df1['continent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the different dataframes that we will use for modeling\n",
    "\n",
    "df_asia = df1[df1['continent'] == 'Asia']\n",
    "df_eu = df1[df1['continent'] == 'Europe']\n",
    "df_af = df1[df1['continent'] == 'Africa']\n",
    "df_oc = df1[df1['continent'] == 'Oceania']\n",
    "df_na = df1[df1['continent'] == 'North America']\n",
    "\n",
    "df_sa = df1[df1['continent'] == 'South America']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the total number of missing values by continent\n",
    "\n",
    "data = [('Asia', df_asia), ('Europe', df_eu), ('Africa', df_af), \n",
    "        ('Oceana', df_oc), ('North America', df_na), ('South America', df_sa)]\n",
    "\n",
    "for (name, df) in data:\n",
    "    nulls = df.isna().sum().sum()\n",
    "    print(name, nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the countries working with in the Asia subset\n",
    "\n",
    "df2_asia = df2[df2['continent'] == 'Asia']\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(df2_asia['longitude'], df2_asia['latitude'])]\n",
    "gdf = GeoDataFrame(df2_asia[['longitude', 'latitude']], geometry=geometry)\n",
    "\n",
    "asia = world[world['continent'] == 'Asia']\n",
    "gdf.plot(ax=asia.plot(figsize=(10, 6)), marker='o', color='red', markersize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning \n",
    "\n",
    "In the below section the project handles the missing values. We first take a look at what percent of each variable is missing as well as what percent of each country is missing before looking at a visual of the number of missing values by variable by date.  This, along with the context of each variable, gives us an idea of how to appropriate deal with the missing values.  See below for the details \n",
    "\n",
    "#### Train/Validate/Test Split? \n",
    "\n",
    "The true train, validate, test split will occur after we aggregate the data to one row per day entry.  However, we do want to keep this split in mind to avoid any unnecessary data leakage.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at dates where train, validate, test split will be\n",
    "\n",
    "unique_dates_total = len(df_asia['date'].unique())\n",
    "first_80_perc = unique_dates_total * .8\n",
    "train_end = df_asia['date'].unique()[int(first_80_perc)]\n",
    "print('Last train date:', train_end)\n",
    "\n",
    "val_perc = first_80_perc + (unique_dates_total * .1)\n",
    "val_end = df_asia['date'].unique()[int(val_perc)]\n",
    "print('Last validation date:', val_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check percent missing by variable\n",
    "num_rows = len(df_asia['date'])\n",
    "for var in df_asia.columns:\n",
    "    missing = df_asia[var].isna().sum()\n",
    "    if missing > 0:\n",
    "        print(var, '{}%'.format(round(missing/num_rows*100, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check percent missing by country\n",
    "for country in df_asia['location'].unique():\n",
    "    mask = df_asia['location'] == country\n",
    "    num_cells = len(df_asia[mask]) * 30\n",
    "    missing = df_asia[mask].isna().sum().sum()\n",
    "    percent = round(missing/num_cells*100, 1)\n",
    "    if percent > 40:\n",
    "        print(country, '{}%'.format(percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize number of missing values by variable by date\n",
    "\n",
    "fig, axes = plt.subplots(ncols=6, nrows=5, figsize=(20,10), sharey=True, sharex=True)\n",
    "\n",
    "for i,var in enumerate(df_asia.columns):\n",
    "    y = []\n",
    "    for date in df_asia['date'].unique():\n",
    "        y.append(df_asia[df_asia['date'] == date][var].isna().sum())\n",
    "    \n",
    "    row = i // 6\n",
    "    col = i % 6\n",
    "    ax = axes[row, col]   \n",
    "    \n",
    "    ax.plot(df_asia['date'].unique(), y)\n",
    "    ax.set_title(var)\n",
    "    ax.set_xticks([])\n",
    "plt.suptitle('Number of Nulls by Variable by Date', fontsize='xx-large');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above information, we will move forward with:\n",
    "- Dropping the below as too much data is missing:\n",
    "    - Country: Northern Cyrus, Macao, and Hong Kong (while not quite 50% missing, Hong Kong is missing quite a lot and is a small regional area -- can be lumped in with China). \n",
    "    - Variables: `icu_patients`, `hosp_patients`, `new_tests_smoothed`, `positive_rate`, `tests_units`, `total_boosters`, \n",
    "- Furthermore Dropping:\n",
    "    - `reproduction_rate` -- as the value cannot be appropriately estimated in the validation and test sets when a vast majority of the countries stopped reporting on this value.\n",
    "    - `new_people_vaccinated_smoothed` -- as we will use new vaccinations smoothed instead.\n",
    "- Drop the below as they will be constant for every day (thus not useful for time series modeling):\n",
    "    - `population_density`, `median_age`, `aged_70_older`, `gdp_per_capita`, `extreme_poverty`, `cardiovasc_death_rate`, `diabetes_prevalence`, `female_smokers`, `male_smokers`, `handwashing_facilities`, `hospital_beds_per_thousand`, `life_expectancy`, `human_development`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop countries\n",
    "mask = (df_asia['location'] == 'Northern Cyprus') | (df_asia['location'] == 'Hong Kong') | (df_asia['location'] == 'Macao')\n",
    "df_asia = df_asia[~mask]\n",
    "    \n",
    "# Drop variables\n",
    "col_to_drop = ['icu_patients', 'hosp_patients', 'new_tests_smoothed', 'positive_rate', 'tests_units', 'reproduction_rate',\n",
    "              'new_people_vaccinated_smoothed', 'total_boosters', 'extreme_poverty', 'handwashing_facilities',\n",
    "              'population_density', 'gdp_per_capita', 'diabetes_prevalence', 'female_smokers', 'male_smokers', \n",
    "              'hospital_beds_per_thousand', 'human_development_index']\n",
    "\n",
    "df_asia = df_asia.drop(col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck missing values be variable\n",
    "num_rows = len(df_asia['date'])\n",
    "for var in df_asia.columns:\n",
    "    missing = df_asia[var].isna().sum()\n",
    "    if missing > 0:\n",
    "        print(var, '{}%'.format(round(missing/num_rows*100, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck missing values by country\n",
    "for country in df_asia['location'].unique():\n",
    "    mask = df_asia['location'] == country\n",
    "    num_cells = len(df_asia[mask]) * 30\n",
    "    missing = df_asia[mask].isna().sum().sum()\n",
    "    percent = round(missing/num_cells*100, 1)\n",
    "    if percent > 20:\n",
    "        print(country, '{}%'.format(percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable | How to Handel missing values | \n",
    "|:---|:--|\n",
    "| `new_cases_smoothed` | Input 0.  NaN only entered when negative values present due to corrective reporting. | \n",
    "| `new_deaths_smoothed` | Input 0. NaN only entered when negative values present due to corrective reporting. | \n",
    "| `people_vaccinated` | Forward fill for missing values after vaccine released by country. | \n",
    "| `new_vaccinations_smoothed` | Input 0. Missing values prior to vaccine release date are 0. Assume no new vaccinations or reporting error for NaN values after vaccine date. | \n",
    "| `stringency_index` | By country, forward fill. Assuming a missing value means it was not reported and no change between previous assigned value. For validation & test sets with no information to forward, assume 0 value (many Covid-19 policies ended in 2023).| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input 0 for assigned columns\n",
    "    # True for train, validate, and test groups -- no need to split\n",
    "fill_values = {'new_cases_smoothed' : 0, 'new_deaths_smoothed':0, 'new_vaccinations_smoothed':0}\n",
    "df_asia.fillna(fill_values, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# People Vaccinated -- Forward fill by country (start with 0)\n",
    "\n",
    "# Start with 0\n",
    "for country in df_asia['location'].unique():\n",
    "    mask = (df_asia['location'] == country)\n",
    "    mask2 = (df_asia['date'] == (df_asia.loc[mask, 'date'].unique().min()))\n",
    "    mask_all = mask & mask2\n",
    "    df_asia.loc[mask_all, 'people_vaccinated'] = df_asia.loc[mask, 'people_vaccinated'].fillna(0)\n",
    "\n",
    "# Train Set\n",
    "for country in df_asia['location'].unique():\n",
    "    mask = (df_asia['location'] == country) & (df_asia['date'] < train_end)\n",
    "    df_asia.loc[mask, 'people_vaccinated'] = df_asia.loc[mask, 'people_vaccinated'].ffill()\n",
    "    if df_asia.loc[mask, 'people_vaccinated'].isna().sum() > 0:\n",
    "        print('train', country)\n",
    "        \n",
    "# Validation Set\n",
    "for country in df_asia['location'].unique():\n",
    "    mask = (df_asia['location'] == country) & (df_asia['date'] >= train_end) & (df_asia['date'] < val_end)\n",
    "    df_asia.loc[mask, 'people_vaccinated'] = df_asia.loc[mask, 'people_vaccinated'].ffill()  \n",
    "    if df_asia.loc[mask, 'people_vaccinated'].isna().sum() > 0:\n",
    "        print('val', country)\n",
    "        \n",
    "# Test Set\n",
    "for country in df_asia['location'].unique():\n",
    "    mask = (df_asia['location'] == country) & (df_asia['date'] >= val_end)\n",
    "    df_asia.loc[mask, 'people_vaccinated'] = df_asia.loc[mask, 'people_vaccinated'].ffill()  \n",
    "    if df_asia.loc[mask, 'people_vaccinated'].isna().sum() > 0:\n",
    "        print('test', country)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three options here: \n",
    "\n",
    "1. Back fill from known values in Validation + Test Sets \n",
    "   - This would avoid any data leakage from the training set into the validation + test sets but not be quite as accurate \n",
    "\n",
    "2. Forward fill information from train set into validation + test sets \n",
    "    - This would be much more accurate (this is a cumulative value per country and thus be the more accurate approach, but would mean data leakage is occurring. \n",
    "\n",
    "3. Drop the variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try back filling\n",
    "\n",
    "# Validation Set\n",
    "for country in df_asia['location'].unique():\n",
    "    mask = (df_asia['location'] == country) & (df_asia['date'] >= train_end) & (df_asia['date'] < val_end)\n",
    "    df_asia.loc[mask, 'people_vaccinated'] = df_asia.loc[mask, 'people_vaccinated'].bfill()  \n",
    "    if df_asia.loc[mask, 'people_vaccinated'].isna().sum() > 0:\n",
    "        print('val', country)\n",
    "        \n",
    "# Test Set\n",
    "for country in df_asia['location'].unique():\n",
    "    mask = (df_asia['location'] == country) & (df_asia['date'] >= val_end)\n",
    "    df_asia.loc[mask, 'people_vaccinated'] = df_asia.loc[mask, 'people_vaccinated'].bfill()  \n",
    "    if df_asia.loc[mask, 'people_vaccinated'].isna().sum() > 0:\n",
    "        print('test', country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That did not work -- rather than dropping the variable we will forward fill completely. \n",
    "    # Yes this will mean a bit of data leakage, but it is reasonable to assume that if we knew the previous number of total\n",
    "    # people vaccinated that this number will be the same moving forward if no new reports/updates made (no new vaccinations)\n",
    "        \n",
    "for country in df_asia['location'].unique():\n",
    "    mask = (df_asia['location'] == country) \n",
    "    df_asia.loc[mask, 'people_vaccinated'] = df_asia.loc[mask, 'people_vaccinated'].ffill()\n",
    "    \n",
    "df_asia['people_vaccinated'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stringency_index\n",
    "df_asia['stringency_index'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at when the missing values for stringency_index are\n",
    "y = []\n",
    "for date in df_asia['date'].unique():\n",
    "    y.append(df_asia[df_asia['date'] == date]['stringency_index'].isna().sum())\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df_asia['date'].unique(), y)\n",
    "ax.set_title('Number of Missing Values in Stringency Index')\n",
    "ax.set_ylabel('Number of Missing Values')\n",
    "ax.set_xlabel('Date');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at average stringency_index by date\n",
    "y = []\n",
    "for date in df_asia['date'].unique():\n",
    "    avg = np.mean(df_asia[df_asia['date'] == date]['stringency_index'])\n",
    "    y.append(avg)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df_asia['date'].unique(), y)\n",
    "ax.set_title('Average Stringency Index')\n",
    "ax.set_ylabel('Stringency Index')\n",
    "ax.set_xlabel('Date');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Forward Fill nulls\n",
    "\n",
    "# Train Set\n",
    "for country in df_asia['location'].unique():\n",
    "    mask = (df_asia['location'] == country) & (df_asia['date'] < train_end)\n",
    "    df_asia.loc[mask, 'stringency_index'] = df_asia.loc[mask, 'stringency_index'].ffill()\n",
    "    if df_asia.loc[mask, 'stringency_index'].isna().sum() > 0:\n",
    "        print('train', country)\n",
    "        \n",
    "# Validation Set\n",
    "for country in df_asia['location'].unique():\n",
    "    mask = (df_asia['location'] == country) & (df_asia['date'] >= train_end) & (df_asia['date'] < val_end)\n",
    "    df_asia.loc[mask, 'stringency_index'] = df_asia.loc[mask, 'stringency_index'].ffill()  \n",
    "    if df_asia.loc[mask, 'stringency_index'].isna().sum() > 0:\n",
    "        print('val', country)\n",
    "        \n",
    "# Test Set\n",
    "for country in df_asia['location'].unique():\n",
    "    mask = (df_asia['location'] == country) & (df_asia['date'] >= val_end)\n",
    "    df_asia.loc[mask, 'stringency_index'] = df_asia.loc[mask, 'stringency_index'].ffill()  \n",
    "    if df_asia.loc[mask, 'stringency_index'].isna().sum() > 0:\n",
    "        print('test', country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we have three countries where the stringency index was never reported, and, as seen with `people_vaccinated`, \n",
    "    # most countries stopped reporting on this value starting in 2023.\n",
    "\n",
    "# For the countries that never reported we will put the average for the day in -- this value will be averaged in aggregate\n",
    "\n",
    "for date in df_asia.loc[df_asia['date']<train_end,'date'].unique():\n",
    "    \n",
    "    if df_asia[df_asia['date'] == date]['stringency_index'].isna().sum() > 0:\n",
    "        avg = np.mean(df_asia[df_asia['date'] == date]['stringency_index'])\n",
    "        \n",
    "        for country in df_asia[df_asia['stringency_index'].isna()]['location'].unique():\n",
    "            mask = (df_asia['location'] == country) & (df_asia['date'] == date)\n",
    "            df_asia.loc[mask, 'stringency_index'].fillna(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Validation + test sets again, some options: \n",
    "\n",
    "1. Back fill from known values in Validation + Test Sets (although as we saw above this might not work if ALL values are missing for each country. \n",
    "2. Forward fill information from train set into validation + test sets \n",
    "    - This would be much more accurate (this is a cumulative value per country and thus be the more accurate approach, but would mean data leakage is occurring. \n",
    "\n",
    "3. Input 0 for this variable in Validation + Test Sets \n",
    "    - This is reasonable only because we know that most countries dropped most if not all of their Covid-19 protections by 2023. \n",
    "4. Drop the variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will try option 1 and if that fails move to option 3\n",
    "\n",
    "# Try backfilling:\n",
    "# Validation Set\n",
    "for country in df_asia['location'].unique():\n",
    "    mask = (df_asia['location'] == country) & (df_asia['date'] >= train_end) & (df_asia['date'] < val_end)\n",
    "    df_asia.loc[mask, 'stringency_index'] = df_asia.loc[mask, 'stringency_index'].bfill()  \n",
    "    if df_asia.loc[mask, 'stringency_index'].isna().sum() > 0:\n",
    "        print('val', country)\n",
    "        \n",
    "# Test Set\n",
    "for country in df_asia['location'].unique():\n",
    "    mask = (df_asia['location'] == country) & (df_asia['date'] >= val_end)\n",
    "    df_asia.loc[mask, 'stringency_index'] = df_asia.loc[mask, 'stringency_index'].bfill()  \n",
    "    if df_asia.loc[mask, 'stringency_index'].isna().sum() > 0:\n",
    "        print('test', country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if still nulls present\n",
    "df_asia['stringency_index'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That clearly did not work -- move forward with filling nulls with 0 for 2023 onwards (validation + test sets)\n",
    "df_asia['stringency_index'] = df_asia['stringency_index'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "df_asia['stringency_index'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Total Missing Values\n",
    "df_asia.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Data + Explore\n",
    "\n",
    "The next subsection aggregates the data such that there is one row per day.  This will give us an overall average of Asia as a continent and allow us to make general predictions and recommendations on a larger geographical scale. \n",
    "\n",
    "The project then goes on and looks at each variable against time, both as the aggregate value and by individual country to further comprehend what data we are working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at data\n",
    "df_asia.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable | How to aggregate |\n",
    "|:--|:--|\n",
    "|`continent` | drop |\n",
    "| `location` | drop |\n",
    "| `date` | unique value per day | \n",
    "| `new_cases_smoothed` | sum per day |\n",
    "| `new_deaths_smoothed` | sum per day |\n",
    "| `people_vaccinated` | sum per day (cumulative value) |\n",
    "| `new_vaccinations_smoothed` | sum per day |\n",
    "| `stringency_index` | average per day |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to create new data frame for time series\n",
    "asia_ts_data = {}\n",
    "asia_ts_data['date'] = df_asia['date'].unique()\n",
    "\n",
    "# Average per day\n",
    "avg_daily = ['stringency_index']\n",
    "\n",
    "for var in avg_daily:\n",
    "    values_to_add = []\n",
    "    \n",
    "    for date in df_asia['date'].unique():\n",
    "        mask = df_asia['date'] == date\n",
    "        avg = np.mean(df_asia.loc[mask, var])\n",
    "        values_to_add.append(avg)\n",
    "        \n",
    "    asia_ts_data[var] = values_to_add\n",
    "    \n",
    "# Sum per day\n",
    "sum_daily = ['new_cases_smoothed', 'new_deaths_smoothed', 'people_vaccinated', 'new_vaccinations_smoothed']\n",
    "\n",
    "for var in sum_daily:\n",
    "    values_to_add = []\n",
    "    \n",
    "    for date in df_asia['date'].unique():\n",
    "        mask = df_asia['date'] == date\n",
    "        sum_ = np.sum(df_asia.loc[mask, var])\n",
    "        values_to_add.append(sum_)\n",
    "        \n",
    "    asia_ts_data[var] = values_to_add        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe\n",
    "df_asia_ts = pd.DataFrame(asia_ts_data)\n",
    "df_asia_ts.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_asia_ts.loc['2020-12-20':'2021-01-15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at new_deaths_smoothed -- our target variable!\n",
    "fig, ax = plt.subplots()\n",
    "df_asia_ts['new_deaths_smoothed'].plot(ax=ax)\n",
    "ax.set_title('New Deaths')\n",
    "ax.set_ylabel('Number of Deaths');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at new_deaths_smoothed by country\n",
    "fig, axes = plt.subplots(ncols=6, nrows=8, figsize=(15,12), sharey=True, sharex=True)\n",
    "\n",
    "for i,country in enumerate(df_asia['location'].unique()):\n",
    "    row = i // 6 \n",
    "    col = i % 6\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    mask = df_asia['location'] == country\n",
    "    x = df_asia.loc[mask, 'date'].unique()\n",
    "    y = df_asia.loc[mask, 'new_deaths_smoothed']\n",
    "    \n",
    "    ax.plot(x, y)\n",
    "    ax.set_title(country)\n",
    "    ax.set_xticks([])\n",
    "plt.suptitle('New Deaths by Country by Date', fontsize='x-large');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The spike in China in January 2023 is due to the end of 'zero COVID' standard.\n",
    "- The spike in India in June of 2021 is due in part to the Delta Variant that was rampant at the time.\n",
    "- Even with these known explanations for the spikes we are seeing, they are still notable outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at new_cases_smoothed aggregated\n",
    "df_asia_ts['new_cases_smoothed'].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at new_cases_smoothed by country\n",
    "\n",
    "fig, axes = plt.subplots(ncols=6, nrows=8, figsize=(15,15), sharey=True, sharex=True)\n",
    "\n",
    "for i,country in enumerate(df_asia['location'].unique()):\n",
    "    row = i // 6 \n",
    "    col = i % 6\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    mask = df_asia['location'] == country\n",
    "    x = df_asia.loc[mask, 'date'].unique()\n",
    "    y = df_asia.loc[mask, 'new_cases_smoothed']\n",
    "    \n",
    "    ax.plot(x, y)\n",
    "    ax.set_title(country)\n",
    "    ax.set_xticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can attribute this spike seen in 2023 in China to the end of 'Zero-Covid' policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at new_vaccinations_smoothed aggregated\n",
    "df_asia_ts['new_vaccinations_smoothed'].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at new_vaccinations smoothed by country\n",
    "\n",
    "fig, axes = plt.subplots(ncols=6, nrows=8, figsize=(15,15), sharey=True, sharex=True)\n",
    "\n",
    "for i,country in enumerate(df_asia['location'].unique()):\n",
    "    row = i // 6 \n",
    "    col = i % 6\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    mask = df_asia['location'] == country\n",
    "    x = df_asia.loc[mask, 'date'].unique()\n",
    "    y = df_asia.loc[mask, 'new_vaccinations_smoothed']\n",
    "    \n",
    "    ax.plot(x, y)\n",
    "    ax.set_title(country)\n",
    "    ax.set_xticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at people_vaccinated aggreggated\n",
    "df_asia_ts['people_vaccinated'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is an unusual trend at the end there that we would not expect to see as this is a cumulative value -- it should never be smaller than its previous value.  Let's investigate this further and fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the last 8 days in the dataset\n",
    "df_asia_ts['people_vaccinated'][-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some countries must not have data for the last 5 days which is causing the averages to produce unexpected \n",
    "# Will move forward with using the value found on the 18th as the last value for these 5 days.\n",
    "\n",
    "list_ = []\n",
    "list_.append(df_asia_ts['people_vaccinated'][-8])\n",
    "list_ = list_ * 5\n",
    "df_asia_ts['people_vaccinated'][-5:] = list_\n",
    "\n",
    "df_asia_ts['people_vaccinated'][-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at people_vaccinated by country\n",
    "fig, axes = plt.subplots(ncols=6, nrows=8, figsize=(15,15), sharey=True, sharex=True)\n",
    "\n",
    "for i,country in enumerate(df_asia['location'].unique()):\n",
    "    row = i // 6 \n",
    "    col = i % 6\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    mask = df_asia['location'] == country\n",
    "    x = df_asia.loc[mask, 'date'].unique()\n",
    "    y = df_asia.loc[mask, 'people_vaccinated']\n",
    "    \n",
    "    ax.plot(x, y)\n",
    "    ax.set_title(country)\n",
    "    ax.set_xticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at stringency_index aggregated\n",
    "df_asia_ts['stringency_index'].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at stringency_index by country\n",
    "fig, axes = plt.subplots(ncols=6, nrows=8, figsize=(15,15), sharey=True, sharex=True)\n",
    "\n",
    "for i,country in enumerate(df_asia['location'].unique()):\n",
    "    row = i // 6 \n",
    "    col = i % 6\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    mask = df_asia['location'] == country\n",
    "    x = df_asia.loc[mask, 'date'].unique()\n",
    "    y = df_asia.loc[mask, 'stringency_index']\n",
    "    \n",
    "    ax.plot(x, y)\n",
    "    ax.set_title(country)\n",
    "    ax.set_xticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "The project goes through various different models:\n",
    "\n",
    "1. Naive Model (Baseline)\n",
    "2. ARIMA (using auto_arima to find best parameters)\n",
    "3. Multivariate ARIMA\n",
    "4. Linear Regression\n",
    "5. Prophet \n",
    "6. Multivariate Prophet\n",
    "\n",
    "All of the above models, aside from Linear Regression, are traditionally used for time series modeling.  The Linear Regression, while not traditionally used for forecasting future date predictions, will be a good comparison.\n",
    "\n",
    "Each model makes a prediction for the validation set dates, and then is evaluated with the mean absolute error and root mean squared error.  Both of these metrics are commonly used when evaluating time series models.  We will be looking at both metrics as RMSE gives greater punishment to larger errors, which is generally good, however it does make it more sensitive to outliers.  Looking at our data, we clearly have identified some unusual spikes. Thus we will also take a look at the MAE as this more robust to outliers. \n",
    "\n",
    "Each model is compared using this error prior to choosing the final model to evaluate with the test set.  We also look at the AIC for the model when applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validate/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Train set will consist of approximately the first {} dates'.format(len(df_asia_ts.index) * .8))\n",
    "print('Validate set will consist of approximately the next {} dates'.format(len(df_asia_ts.index) * .1))\n",
    "print('Test set will consist of approximately the last {} dates'.format(len(df_asia_ts.index) * .1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Set\n",
    "first_80_perc = len(df_asia_ts.index) * .8\n",
    "mask = df_asia_ts.index[:int(first_80_perc)]\n",
    "train = df_asia_ts.loc[mask]\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Set\n",
    "val_perc = 139 + int(first_80_perc)\n",
    "mask = df_asia_ts.index[int(first_80_perc):val_perc]\n",
    "validate = df_asia_ts.loc[mask]\n",
    "validate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set\n",
    "mask = df_asia_ts.index[val_perc:]\n",
    "test = df_asia_ts.loc[mask]\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline -- Naive Model\n",
    "\n",
    "The Naive Method is to predict that the value tomorrow will be the same value that is seen today.  Thus we will shift our data by one for our 'predictions'.  We also look a bit further into our data, looking at the rolling standard deviation and variance of the residuals prior to forecasting  for the validation dates and evaluating the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Naive \"Model\"\n",
    "baseline = train['new_deaths_smoothed'].shift(1)\n",
    "\n",
    "# Visualize f\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "train['new_deaths_smoothed'].plot(ax=ax, c='b', label='train data')\n",
    "baseline.plot(ax=ax, c='r', label='shifted data')\n",
    "ax.set_title('Naive Predictions - Train')\n",
    "ax.set_ylabel('Number of Deaths')\n",
    "ax.set_xlabel('Date')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RMSE\n",
    "baseline_error_train = np.sqrt(mean_squared_error(train['new_deaths_smoothed'][1:], \n",
    "                                                  baseline.dropna()))\n",
    "baseline_error_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for trends -- want residuals to look like white noise\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "residuals = baseline[1:] - train['new_deaths_smoothed'][1:]\n",
    "ax.plot(residuals.index, residuals, label='residuals')\n",
    "ax.plot(residuals.index, residuals.rolling(30).std(), label='rolling std')\n",
    "ax.set_xticks([])\n",
    "ax.set_title('Residuals')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check variance\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(residuals.index, residuals.rolling(30).var())\n",
    "ax.set_xticks([])\n",
    "ax.set_title('Rolling Variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Predict\" -- use last known data point for all future predictions\n",
    "y_preds_baseline = []\n",
    "y_preds_baseline.append(baseline[-1])\n",
    "y_preds_baseline = y_preds_baseline * len(validate.index)\n",
    "\n",
    "# For evaluation, assign y_train and y_val\n",
    "y_train = train['new_deaths_smoothed']\n",
    "y_val = validate['new_deaths_smoothed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_preds(train_data, validate_data, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    This function takes in the values the model was trained on, and the true validation values, \n",
    "    both as pandas series with the date as the index. It also takes in the predictions as a list \n",
    "    or pandas series, and graphs all three on a single axis.\n",
    "    \n",
    "    Expected input: \n",
    "        train_data | pandas series, with datetime as index, of the target training data\n",
    "        validate_data | pandas series, with datetime as index, of the target validation data\n",
    "        y_pred | list of the target predictions\n",
    "        model_name | string\n",
    "        \n",
    "    Expected output: matplotlib graph of the target training data, validation data, and predictions \n",
    "        plotted against the date\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    validate_data.plot(ax=ax, color='b', label='validation data')\n",
    "    train_data.plot(ax=ax, color='g', label='train data')\n",
    "    ax.plot(validate_data.index, y_pred, color='r', label='prediction')\n",
    "    ax.set_title(model_name + ' Predictions')\n",
    "    ax.set_ylabel('Number of Deaths')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "graph_preds(y_train, y_val, y_preds_baseline, 'Naive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record Evaluation Results\n",
    "\n",
    "all_results = {'model': [], 'rmse':[], 'mae':[],'train_aic':[]}\n",
    "\n",
    "def evaluate(y_true, y_pred, all_results, model_name, aic):\n",
    "    \"\"\"\n",
    "    This function takes in a y_true, y_pred, a dictionary called all_results, and a model name. \n",
    "    It calculates the RMSE, prints it, and appends it and the model_name to the appropriate dictionary key.\n",
    "    \n",
    "    Expected input:\n",
    "        y_true | true target values as a pandas series or list\n",
    "        y_pred | target predictions as a pandas series or list\n",
    "        all_results | python dictionary\n",
    "        model_name | string\n",
    "        aic | number or np.nan\n",
    "        \n",
    "    Expected output: print out of the updated dictionary, all_results\n",
    "    \"\"\"\n",
    "    # RMSE\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    \n",
    "    # MAE\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    # Add to dictionary\n",
    "    all_results['model'].append(model_name)\n",
    "    all_results['rmse'].append(rmse)\n",
    "    all_results['mae'].append(mae)\n",
    "    all_results['train_aic'].append(aic)\n",
    "    \n",
    "    print(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate(y_val, y_preds_baseline, all_results, 'Naive', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - ARIMA\n",
    "\n",
    "Before jumping into the auto_arima function which performs a grid-search-like function internally to achieve the optimal p, d, and q values, we take a look at the ACF and PACF graphs (for both the original data as well as the once differenced data) to better understand the time series data.  We also visualize the differenced data and perform a Dickey-Fuller test to ascertain whether or not the differenced data is stationary. \n",
    "\n",
    "After that manual work, we move forward into utilizing the auto_arima function which does that work for us.  The model is created and then evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF\n",
    "fig, ax = plt.subplots()\n",
    "plot_acf(train['new_deaths_smoothed'], ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first difference -- now plot ACF\n",
    "fig, ax =  plt.subplots()\n",
    "data = train['new_deaths_smoothed'].diff().dropna()\n",
    "plot_acf(data, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACF (correlation between current time periord + lags \n",
    "    # accounting for the correlation between the intermediate time periods)\n",
    "fig, ax = plt.subplots()\n",
    "plot_pacf(train['new_deaths_smoothed'], ax=ax, method='ols-adjusted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACF of differenced data\n",
    "fig, ax =  plt.subplots()\n",
    "data = train['new_deaths_smoothed'].diff().dropna()\n",
    "plot_pacf(data, ax=ax, method='ols-adjusted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Differenced Data\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train['new_deaths_smoothed'].diff().dropna())\n",
    "ax.set_title('Differenced Data')\n",
    "ax.set_xticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the once differenced data is stationary\n",
    "\n",
    "p_val = adfuller(train['new_deaths_smoothed'].diff()[1:])[1]\n",
    "print(f\"The p-value associated with the Dickey-Fuller statistical test is {p_val},\")\n",
    "\n",
    "if p_val < 0.05:\n",
    "    print(\"Thus we can safely assume that the differenced data is stationary.\")\n",
    "else:\n",
    "    print(\"Thus we cannot reject the null hypothesis that the differenced data is \\\n",
    "not stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the adjusted Dickey-Fuller test, we can assert that the once differenced data is stationary which means it is suitable for modeling.  However, based on the ACF and PACF visuals, even of the differenced data, we know we will need to include some autoregressive (AR) and moving average (MA) terms.\n",
    "\n",
    "Let's also take a look at the data once again to see if we should include any seasonality terms in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for seasonality trends\n",
    "y_train.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not appear to be any seasonality trends present in our data (on the weekly, monthly, nor yearly scale).  We will move forward with excluding these features from our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto_arima function parameters -- values chosen + notes\n",
    "\n",
    "# time series to train on + predict\n",
    "y_train = train['new_deaths_smoothed']\n",
    "# no constants\n",
    "X_train = train[['new_cases_smoothed', 'people_vaccinated', 'new_vaccinations_smoothed', 'stringency_index']]\n",
    "\n",
    "# AR term\n",
    "start_p = 1\n",
    "max_p = 5 # default\n",
    "\n",
    "# Order of non-seasonal first-differencing\n",
    "d=None # default -- calculated\n",
    "max_d = 2 # default -- from above we know 1 difference is stationary \n",
    "\n",
    "# MA term\n",
    "start_q = 1\n",
    "max_q = 5 #default\n",
    "\n",
    "# Seasonality \n",
    "seasonal = False \n",
    "\n",
    "# Evaluaton to select best ARIMA model\n",
    "information_criterion = 'aic' # default\n",
    "\n",
    "# Level of significance\n",
    "alpha = 0.05 # default\n",
    "\n",
    "# Scoring default is MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA Model\n",
    "arima = auto_arima(y_train, start_p=start_p, start_q=start_q, seasonal=False, trace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future dates to forecast\n",
    "n_periods_pred = validate['new_deaths_smoothed'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_preds_arima = arima.predict(n_periods=n_periods_pred)\n",
    "\n",
    "# Visualize\n",
    "graph_preds(y_train, y_val, y_preds_arima, 'ARIMA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIC\n",
    "arima_aic = arima.aic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate(y_val, y_preds_arima, all_results, 'ARIMA', arima_aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our RMSE and MAE have both improved from our baseline predictions.  Let's see if we can improve that even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Multivariate ARIMA\n",
    "\n",
    "Rather than just use the target variable, the true 'time series' data of `new_deaths_smoothed`, we will incorporate the other variables that we have available to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate ARIMA model\n",
    "arima_multi = auto_arima(y_train, X_train, start_p=start_p, start_q=start_q, seasonal=False, trace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_multi.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "X_val = validate[['new_cases_smoothed', 'people_vaccinated', 'new_vaccinations_smoothed', 'stringency_index']]\n",
    "y_preds_arima_multi = arima_multi.predict(n_periods=n_periods_pred, X=X_val)\n",
    "\n",
    "#visualize\n",
    "graph_preds(y_train, y_val, y_preds_arima_multi, 'Mulltivariate ARIMA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AIC\n",
    "multi_arima_aic = arima_multi.aic()\n",
    "\n",
    "# evaluate\n",
    "evaluate(y_val, y_preds_arima_multi, all_results, 'ARIMA Multivariate', multi_arima_aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see improvement in both RMSE and MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - Linear Regresion\n",
    "\n",
    "While Linear Regressions are typically not suited for time series data, there is no harm in trying it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create + fit the model\n",
    "lr = OLS(y_train, sm.api.add_constant(X_train))\n",
    "lr_results = lr.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at summary of model\n",
    "print(lr_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_preds_lr = lr_results.predict(sm.api.add_constant(X_val))\n",
    "\n",
    "# Visualize\n",
    "graph_preds(y_train, y_val, y_preds_lr, 'Linear Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the partial regression grid for our variables\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "sm.api.graphics.plot_partregress_grid(lr_results, \n",
    "                                      exog_idx=list(X_train.columns.values),\n",
    "                                      grid=(2,2),\n",
    "                                      fig=fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to see here that the relationship between the number of deaths and the number of new vaccinations is positive here.  Intuitively, one would predict that as the total number of people vaccinated increases, the number of deaths caused by the disease would decrease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIC\n",
    "lr_aic = lr_results.aic\n",
    "\n",
    "# evaluate\n",
    "evaluate(y_val, y_preds_lr, all_results, 'Linear Regression', lr_aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supprisingly, our Linear Regression model is the most performant model yet!  Let's try Facebook's Prophet last to see if we can improve even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5 - Facebook's Prophet\n",
    "\n",
    "The prophet library imposes strict conditions for the input column names, so firstly, some reformatting is done. Then the model is created, visualized (the prophet library has some interesting built in visualizations), and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat for Prophet\n",
    "data_train = {'ds': train.index, 'y': y_train.values}\n",
    "train_prophet = pd.DataFrame.from_dict(data_train)\n",
    "\n",
    "data_val = {'ds': validate.index, 'y': y_val.values}\n",
    "val_prophet = pd.DataFrame.from_dict(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model - default uncertainty is 80% - update to 95%\n",
    "prophet_model = Prophet(interval_width=0.95) \n",
    "\n",
    "# Fit Model\n",
    "prophet_model.fit(train_prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using future dates\n",
    "y_preds_prophet = prophet_model.predict(val_prophet[['ds']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_prophet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions with uncertainty\n",
    "prophet_model.plot(y_preds_prophet, uncertainty=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trends\n",
    "prophet_model.plot_components(y_preds_prophet)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Predictions (again)\n",
    "graph_preds(y_train, y_val, y_preds_prophet['yhat'], 'Prophet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate(y_val, y_preds_prophet['yhat'], all_results, 'Prophet', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prophet model is not doing as well as we would like, however this one was only on the time series alone (just the deaths per day data).  Let's create a new prophet model with additional regressors to create a multivariate time series model -- hopefully that will improve our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6 - Multivariate Prophet\n",
    "\n",
    "Again, let's see if we can improve the model by including the other variables in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data\n",
    "train_prophet_multi = pd.concat([train_prophet, X_train.reset_index().drop('date', axis=1)], axis=1)\n",
    "train_prophet_multi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "prophet_multi = Prophet(interval_width=0.95)\n",
    "\n",
    "# Add Regressors for multivariate \n",
    "for var in X_train.columns:\n",
    "    prophet_multi.add_regressor(var, standardize=False)\n",
    "\n",
    "# Fit Model\n",
    "prophet_multi.fit(train_prophet_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Validation Data\n",
    "val_prophet_multi = pd.concat([val_prophet[['ds']], X_val.reset_index().drop('date', axis=1)], axis=1)\n",
    "val_prophet_multi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using future dates\n",
    "y_preds_prophet_multi = prophet_multi.predict(val_prophet_multi)\n",
    "\n",
    "y_preds_prophet_multi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions with uncertainty\n",
    "prophet_multi.plot(y_preds_prophet_multi, uncertainty=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Trends\n",
    "prophet_multi.plot_components(y_preds_prophet_multi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at regressor coefficients + intervals\n",
    "regressor_coefficients(prophet_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions (again)\n",
    "graph_preds(y_train, y_val, y_preds_prophet_multi['yhat'], 'Multivariate Prophet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate(y_val, y_preds_prophet_multi['yhat'], all_results, 'Prophet Multivariate', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df = pd.DataFrame.from_dict(all_results)\n",
    "all_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that all of our models that included further information, the models that did not rely solely on the number of deaths per day, had better results when forecasting. This is intuitive -- more information available generally leads to a better predictive model. \n",
    "\n",
    "We will move forward with the Multivariate Prophet Model as our final model. While the Linear Regression model has a slightly better RMSE, it is forecasting negative values for our test dates which is unreasonable for our situation. The Multivariate Prophet model, more suited to time series, is predicting positive values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Evaluation\n",
    "\n",
    "Now that we have chosen our final model we can evaluate it on the hold out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data\n",
    "train_all = pd.concat([train, validate])\n",
    "\n",
    "y_train_all = train_all[['new_deaths_smoothed']].reset_index()\n",
    "y_train_all.rename(columns={'date': 'ds', 'new_deaths_smoothed':'y'}, inplace=True)\n",
    "\n",
    "X_train_all = train_all[['new_cases_smoothed', 'people_vaccinated', 'new_vaccinations_smoothed', 'stringency_index']]\n",
    "\n",
    "train_all_prophet = pd.concat([y_train_all,\n",
    "                               X_train_all.reset_index().drop('date', axis=1)],\n",
    "                              axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final model\n",
    "prophet_final = Prophet(interval_width=0.95)\n",
    "\n",
    "# Add Regressors for multivariate\n",
    "for var in X_train.columns:\n",
    "    prophet_final.add_regressor(var, standardize=False)\n",
    "    \n",
    "# Fit Model\n",
    "prophet_final.fit(train_all_prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Test Data\n",
    "y_test_prophet = test[['new_deaths_smoothed']].reset_index()\n",
    "y_test_prophet.rename(columns={'date':'ds','new_deaths_smoothed':'y'}, inplace=True)\n",
    "\n",
    "X_test = test[['new_cases_smoothed', 'people_vaccinated', 'new_vaccinations_smoothed', 'stringency_index']]\n",
    "\n",
    "test_prophet = pd.concat([y_test_prophet, X_test.reset_index().drop('date', axis=1)], axis=1)\n",
    "test_prophet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_preds_final = prophet_final.predict(test_prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions with uncertainty\n",
    "prophet_final.plot(y_preds_final, uncertainty=True)\n",
    "plt.title('Final Model Predictions')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Number of Deaths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trends\n",
    "prophet_final.plot_components(y_preds_final)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at regressor coefficients + intervals\n",
    "regressor_coefficients(prophet_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "graph_preds(train_all['new_deaths_smoothed'], \n",
    "            test['new_deaths_smoothed'], \n",
    "            y_preds_final['yhat'],\n",
    "            'Final Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE + MAE Final\n",
    "final_rmse = mean_squared_error(test['new_deaths_smoothed'], y_preds_final['yhat'], squared=False)\n",
    "final_mae = mean_absolute_error(test['new_deaths_smoothed'], y_preds_final['yhat'])\n",
    "print('Final RMSE:', round(final_rmse, 2))\n",
    "print('Final MAE:', round(final_mae, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model performed better on the validation data than it did on the test data.  In part, this is because the pandemic had more or less ended by this time (mid 2023), which means the actual values for our data lie very close to zero, whereas our model is predicting the deaths to increase once again (similar to the actuals of July in 2020).  \n",
    "\n",
    "With an ending root mean squared error of 904 and mean absolute error of only 836, I would say this is a fairly good model, taking into account that this estimation is for the whole continent of Asia rather than one country alone.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion + Recommendations\n",
    "\n",
    "Overall, the project completes what it set out to do. We have created a predictive time series model that will forecast the number of deaths that will occur in the near future.  Having such a model, especially one that takes into consideration factors such as the stringency index which directly relates to the public health policies currently put in place, can be very useful to public health officials.  With this information they can work on when to ramp up or ease off public health ordinances such as mask mandates and social distancing. \n",
    "\n",
    "The recommendations for the business are as follows:\n",
    "\n",
    "\n",
    "1. **Utilize the model to advise public health officials.**  Once we have forecasting the amount of harm predicted, we can then advise public health officials on what actions to take concerning public health measures such as mask mandates, social distancing, and stay-at-home orders (either to make more strict or lessen).\n",
    "\n",
    "\n",
    "2. **Utilize the model to aid in resource planning.**  From our forecasts of when spikes will occur, we can make recommendations  to vendors and hospitals concerning resource planning. \n",
    "\n",
    "\n",
    "3. **Investigate why deaths tend to be reported higher on Fridays.** From our final model, we can see a weekly trend where reported number of deaths is generally higher on Fridays by about 4 people, and lower earlier in the week by about 3 people. While a slight trend, it would be interesting to investigate why this is;  if it is anything in the hospitals systems or public health trends that can be addressed in such a way that number of deaths lessens.\n",
    "\n",
    "\n",
    "\n",
    "###  Next Steps\n",
    "\n",
    "There is always more to do and try!  Below are some ideas for expanding on this project:\n",
    "\n",
    "\n",
    "- Continue to make data more accurate and complete\n",
    "    - The missing, purposefully mis-reported, and unusable data all made our model less accurate than it could have been.  Looking for ways in the future to gain accurate, consistent data, as well as ways to utilize the 'constants' would be beneficial in future model endeavors. \n",
    "- Data by continent\n",
    "    - compare predictive models by continent\n",
    "    - compare how well each continent handled the pandemic\n",
    "- Data by country\n",
    "    - create regressive model that can predict number of deaths based on location (and other variables that were in dataset) *Not Time Series*\n",
    "    - Time series by country and then create a user interface to choose country and date to forecast and then predicts number of deaths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
